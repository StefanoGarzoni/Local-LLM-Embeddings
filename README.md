# Local-LLM-Embeddings
Local LLM Embeddings estrae testo e immagini, genera embedding localmente, li archivia in SQL e usa un LLM per rispondere alle domande. Trova il contesto con similaritÃ  del coseno per un prompt engineering efficace, garantendo privacy e prestazioni senza dipendere dal cloud.

Local LLM Embeddings
Un framework per gestire embedding e ricerca vettoriale in locale con LLM

ğŸ” Introduzione
Local LLM Embeddings Ã¨ un progetto che consente di:

Estrarre testo e immagini da documenti
Generare embedding vettoriali localmente
Archiviare i dati in un database SQL
Eseguire ricerche basate sulla similaritÃ  del coseno
Ottenere risposte contestualizzate tramite un modello di linguaggio
Tutto il processo avviene in locale, senza invio di dati a server esterni, garantendo privacy e controllo totale.

ğŸ› ï¸ FunzionalitÃ 
âœ” Estrazione Dati â€“ Lettura di testo e immagini da documenti (PDF, immagini, ecc.)
âœ” Generazione Embeddings â€“ Creazione di rappresentazioni vettoriali dei testi
âœ” Database SQL â€“ Archiviazione strutturata degli embedding
âœ” Ricerca Vettoriale â€“ Individuazione dei contesti piÃ¹ rilevanti con similaritÃ  del coseno
âœ” Chat con LLM â€“ Risposte migliorate grazie al prompt engineering basato sugli embedding

âš™ï¸ Come Funziona?
1ï¸âƒ£ Estrazione di Testo e Immagini
Due script analizzano documenti in vari formati, estraggono testo e immagini, e li pre-processano per la generazione degli embedding.

2ï¸âƒ£ Creazione degli Embeddings
Gli embedding vengono generati localmente da un LLM, trasformando il testo in vettori numerici. Questi vettori catturano il significato e la relazione semantica tra le parole.

3ï¸âƒ£ Archiviazione nel Database SQL
Un altro script normalizza e memorizza gli embedding in un database SQL, rendendoli facilmente accessibili per ricerche future.

4ï¸âƒ£ Ricerca Vettoriale e SimilaritÃ  del Coseno
Quando l'utente pone una domanda nella chat, viene generato un embedding della query e confrontato con gli embedding memorizzati nel database tramite la similaritÃ  del coseno. Questo permette di individuare i contenuti piÃ¹ rilevanti.

5ï¸âƒ£ Prompt Engineering e Risposta del Modello
Il contesto trovato viene incluso nel prompt passato al LLM, migliorando la qualitÃ  della risposta generata. Questo approccio ottimizza l'efficacia delle risposte senza la necessitÃ  di ricerche esterne.

ğŸ”’ Vantaggi
âœ… Tutto in locale â€“ Nessun rischio di perdita di dati o dipendenza dal cloud
âœ… Maggiore privacy â€“ Nessuna condivisione di documenti sensibili
âœ… Risultati piÃ¹ pertinenti â€“ Grazie alla ricerca vettoriale e al prompt engineering

ğŸ“§ Contatti
Per domande o contributi, apri una Issue o contattami su GitHub! ğŸš€
